% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\section{Supervised Autoencoder-MLP for Image Classification: Exam 1
Final
Report}\label{supervised-autoencoder-mlp-for-image-classification-exam-1-final-report}

\textbf{Course:} DATS 6303 - Deep Learning\\
\textbf{Author:} Venkatesh Nagarjuna\\
\textbf{Date:} October 24, 2025\\
\textbf{Nickname:} Andrew

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Summary}\label{summary}

This report presents a comprehensive analysis of a Supervised
Autoencoder-MLP hybrid architecture developed for multi-class image
classification. The model was designed to address a 10-class
classification problem with severe class imbalance (517:1 ratio) using a
combination of unsupervised feature learning and supervised
classification. After implementing advanced regularization techniques
and class weighting strategies, the model achieved a test accuracy of
\textbf{41.1\%} with Cohen\textquotesingle s Kappa of \textbf{0.30},
indicating performance sufficiently above random baseline for this
severely imbalanced dataset.

\textbf{Key Findings:}

\begin{itemize}
\item
  The model struggles significantly with the extreme class imbalance
  present in the dataset
\item
  Class 5 (majority class) comprises 42.2\% of samples while Class 1 has
  only 0.1\%
\item
  The model exhibits severe prediction bias, over-predicting minority
  classes and under-predicting majority classes
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{1. Introduction}\label{1-introduction}

\subsubsection{1.1 Problem Definition}\label{11-problem-definition}

\textbf{Task:} Multi-class image classification with 10 distinct classes

\textbf{Input:} RGB images of size 224×224 pixels (3 channels),
preprocessed and flattened to dimension 150,528

\textbf{Output:} Classification probabilities across the 10 classes
using softmax activation

\textbf{Challenge:} The dataset exhibits extreme class imbalance with a
ratio of 517:1 between the most frequent class (class5: 11,389 samples,
42\%) and the least frequent class (class1: 22 samples, 0.1\%)

\subsubsection{1.2 Motivation}\label{12-motivation}

Traditional fully-connected neural networks often struggle with
high-dimensional image data and class imbalance. This project explores a
hybrid architecture combining:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Autoencoder component} for unsupervised feature learning and
  dimensionality reduction
\item
  \textbf{MLP component} for supervised classification using both
  compressed and original features
\item
  \textbf{Advanced regularization} including Gaussian noise, dropout, L2
  regularization, and label smoothing
\item
  \textbf{Sample weighting} to address severe class imbalance
\end{enumerate}

The dual-objective approach (reconstruction + classification) was
hypothesized to improve feature representations and for better model
generalization.

\subsubsection{1.3 Objectives}\label{13-objectives}

\begin{itemize}
\item
  Learn robust feature representations through autoencoder pretraining
\item
  Achieve acceptable classification performance despite severe class
  imbalance
\item
  Document model architecture, training process, and performance metrics
  comprehensively
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2. Related Work}\label{2-related-work}

\subsubsection{2.1 Autoencoders for Feature
Learning}\label{21-autoencoders-for-feature-learning}

Autoencoders have been widely used for unsupervised feature learning by
learning compressed representations of input data. The encoder
compresses input to a bottleneck layer while the decoder reconstructs
the original input, forcing the network to learn meaningful
representations.

\subsubsection{2.2 Hybrid Architectures}\label{22-hybrid-architectures}

Supervised autoencoders combine reconstruction and classification
objectives, allowing the model to benefit from both unsupervised feature
learning and supervised task-specific optimization. This approach has
shown promise in scenarios with limited labeled data or high-dimensional
inputs.

\subsubsection{2.3 Class Imbalance
Solutions}\label{23-class-imbalance-solutions}

Common approaches to class imbalance include:

\begin{itemize}
\item
  \textbf{Sample weighting:} Assigning higher weights to minority
  classes during training
\item
  \textbf{Oversampling/undersampling:} Balancing class distributions
  through data augmentation
\item
  \textbf{Focal loss:} Emphasizing hard-to-classify examples
\item
  \textbf{Ensemble methods:} Combining multiple models trained on
  balanced subsets
\end{itemize}

This implementation uses sample weighting with weights calculated as:
\texttt{weight{[}i{]}\ =\ total\_samples\ /\ (n\_classes\ ×\ class\_count{[}i{]})}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{3. Dataset and Features}\label{3-dataset-and-features}

\subsubsection{3.1 Dataset Overview}\label{31-dataset-overview}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Metric & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Total samples & 26,989 (training set) \\
Number of classes & 10 \\
Input dimensions & 150,528 (224×224×3) \\
Image format & RGB, 224×224 pixels \\
Data split & Training/Validation \\
Preprocessing & Min-max normalization (0-1) \\
\end{longtable}
}

\subsubsection{3.2 Class Distribution (Train
Split)}\label{32-class-distribution-train-split}

The test set exhibits severe class imbalance:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Class & Samples & Percentage \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
class1 & 22 & 0.1\% \\
class2 & 2,325 & 8.6\% \\
class3 & 4,439 & 16.4\% \\
class4 & 1,212 & 4.5\% \\
class5 & 11,389 & 42.2\% \\
class6 & 63 & 0.2\% \\
class7 & 1202 & 4.5\% \\
class8 & 4666 & 17.3\% \\
class9 & 935 & 3.5\% \\
class10 & 736 & 2.7\% \\
\end{longtable}
}

\textbf{Imbalance Ratio:} 517.7:1 (class5 vs. class1)

\subsubsection{3.3 Data Preprocessing
Pipeline}\label{33-data-preprocessing-pipeline}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Image Loading:} Images loaded from disk using
  TensorFlow\textquotesingle s \texttt{tf.io.decode\_image}
\item
  \textbf{Resizing:} All images resized to 224×224 pixels using bilinear
  interpolation
\item
  \textbf{Normalization:} Pixel values scaled from {[}0, 255{]} to {[}0,
  1{]} range
\item
  \textbf{Flattening:} Images flattened to 1D vectors of dimension
  150,528
\item
  \textbf{Batching:} Data batched into groups of 32 samples
\item
  \textbf{Prefetching:} Pipeline optimized with AUTOTUNE for efficient
  data loading
\end{enumerate}

\subsubsection{3.4 Memory Optimization}\label{34-memory-optimization}

To prevent out-of-memory errors, the implementation uses
TensorFlow\textquotesingle s \texttt{tf.data.Dataset} API with streaming
data loading:

\begin{itemize}
\item
  Data loaded from disk on-demand rather than loading entire dataset
  into RAM
\item
  Parallel processing enabled with \texttt{AUTOTUNE} for map operations
\item
  Prefetching enabled to overlap data preprocessing with model execution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{4. Model Design}\label{4-model-design}

\subsubsection{4.1 Overall Architecture}\label{41-overall-architecture}

The model implements a \textbf{Supervised Autoencoder-MLP hybrid} with
three parallel branches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Encoder Branch:} Compresses input from 150,528 → 512 → 256 →
  128 dimensions
\item
  \textbf{Decoder Branch:} Reconstructs input from 128 → 256 → 512 →
  150,528 dimensions
\item
  \textbf{Classification Branch:} Uses concatenated features (128 +
  150,528 = 150,656) → 512 → 256 → 128 → 10 classes
\end{enumerate}

\textbf{Total Parameters:} 695,787,808 (2.59 GB)

\begin{itemize}
\item
  Trainable: 231,927,562 (884.73 MB)
\item
  Non-trainable: 5,120 (20.00 KB)
\item
  Optimizer: 463,855,126 (1.73 GB)
\end{itemize}

\subsubsection{4.2 Encoder Branch}\label{42-encoder-branch}

\textbf{Purpose:} Learn compressed representations of input images

\textbf{Architecture:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Input (150,528) }
\NormalTok{  → Gaussian Noise (σ=0.2)}
\NormalTok{  → Dense(512) + BatchNorm + Swish + Dropout(0.2)}
\NormalTok{  → Dense(256) + BatchNorm + Swish + Dropout(0.2)}
\NormalTok{  → Dense(128) + BatchNorm + Swish + Dropout(0.2)}
\NormalTok{  → Encoded Features (128)}
\end{Highlighting}
\end{Shaded}

\textbf{Key Components:}

\begin{itemize}
\item
  \textbf{Gaussian Noise Injection:} Adds robustness by injecting noise
  (std=0.2) at the input
\item
  \textbf{Swish Activation:} \texttt{f(x)\ =\ x\ ·\ sigmoid(βx)} where
  β=1.0, providing smooth non-linearity
\item
  \textbf{Batch Normalization:} Stabilizes training by normalizing
  activations
\item
  \textbf{L2 Regularization:} Weight decay of 0.0001 to prevent
  overfitting
\item
  \textbf{Dropout:} 20\% dropout rate for regularization
\end{itemize}

\subsubsection{4.3 Decoder Branch}\label{43-decoder-branch}

\textbf{Purpose:} Reconstruct original input from compressed features
(unsupervised learning signal)

\textbf{Architecture:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Encoded Features (128)}
\NormalTok{  → Dense(256) + BatchNorm + Swish + Dropout(0.2)}
\NormalTok{  → Dense(512) + BatchNorm + Swish + Dropout(0.2)}
\NormalTok{  → Dense(150,528) + Sigmoid}
\NormalTok{  → Reconstructed Output (150,528)}
\end{Highlighting}
\end{Shaded}

\textbf{Loss Function:} Mean Squared Error (MSE) with weight 0.15

\subsubsection{4.4 Classification Branch
(MLP)}\label{44-classification-branch-mlp}

\textbf{Purpose:} Perform supervised classification using enriched
features

\textbf{Architecture:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Concatenation[Encoded(128) + Original Input(150,528)] = 150,656}
\NormalTok{  → Dense(512) + BatchNorm + Swish + Dropout(0.3)}
\NormalTok{  → Dense(256) + BatchNorm + Swish + Dropout(0.4)}
\NormalTok{  → Dense(128) + BatchNorm + Swish + Dropout(0.3)}
\NormalTok{  → Dense(10) + Softmax}
\NormalTok{  → Classification Output (10 classes)}
\end{Highlighting}
\end{Shaded}

\textbf{Loss Function:} Categorical cross entropy with label smoothing
0.1

\textbf{Key Components:}

\begin{itemize}
\item
  \textbf{Feature Concatenation:} Combines compressed features with
  original input to preserve both high-level and low-level information
\item
  \textbf{Progressive Dropout:} Increasing dropout rates {[}0.3, 0.4,
  0.3{]} to prevent overfitting
\item
  \textbf{Softmax Output:} Produces probability distribution over 10
  classes
\end{itemize}

\subsubsection{4.5 Custom Activation Function:
Swish}\label{45-custom-activation-function-swish}

The model uses Swish activation instead of ReLU:

\[\text{Swish}(x) = x \cdot \sigma(\beta x) = x \cdot \frac{1}{1 + e^{-\beta x}}\]

where \(\beta = 1.0\)

\textbf{Benefits:}

\begin{itemize}
\item
  Smooth, non-monotonic function
\item
  Self-gating mechanism
\item
  Better gradient flow than ReLU
\item
  Unbounded above, bounded below
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{5. Training Configuration}\label{5-training-configuration}

\subsubsection{5.1 Hyperparameters}\label{51-hyperparameters}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Parameter & Value & Rationale \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Optimizer & Adam & Adaptive learning rate for efficient convergence \\
Learning Rate & 0.0001 & Conservative rate to prevent overshooting \\
Batch Size & 32 & Memory-efficient while maintaining gradient
stability \\
Epochs & 100 & With early stopping to prevent overfitting \\
Label Smoothing & 0.1 & Prevents overconfident predictions \\
L2 Regularization & 0.0001 & Weight decay for generalization \\
Gaussian Noise Std & 0.2 & Data augmentation for robustness \\
\end{longtable}
}

\subsubsection{5.2 Multi-Objective Loss
Function}\label{52-multi-objective-loss-function}

The model optimizes two objectives simultaneously:

\[\mathcal{L}_{\text{total}} = \lambda_{\text{class}} \mathcal{L}_{\text{class}} + \lambda_{\text{recon}} \mathcal{L}_{\text{recon}}\]

where:

\begin{itemize}
\item
  \(\mathcal{L}_{\text{class}}\) = Categorical Cross-Entropy with Label
  Smoothing (0.1)
\item
  \(\mathcal{L}_{\text{recon}}\) = Mean Squared Error (MSE)
\item
  \(\lambda_{\text{class}} = 1.0\)
\item
  \(\lambda_{\text{recon}} = 0.15\)
\end{itemize}

\textbf{Label Smoothing Formula:}

\[y_{\text{smooth}}^{(i)} = y^{(i)} (1 - \epsilon) + \frac{\epsilon}{K}\]

where \(\epsilon = 0.1\) and \(K = 10\) classes

\subsubsection{5.3 Class Imbalance
Handling}\label{53-class-imbalance-handling}

\textbf{Sample Weighting:} Each sample receives a weight based on its
class frequency:

\[w_i = \frac{N_{\text{total}}}{K \times N_i}\]

where:

\begin{itemize}
\item
  \(N_{\text{total}}\) = Total number of samples
\item
  \(K\) = Number of classes (10)
\item
  \(N_i\) = Number of samples in class \(i\)
\end{itemize}

This ensures minority classes receive higher weights during training.

\subsubsection{5.4 Training Callbacks}\label{54-training-callbacks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Early Stopping:}

  \begin{itemize}
  \item
    Monitor: \texttt{val\_classification\_output\_loss}
  \item
    Patience: 20 epochs
  \item
    Restore best weights when stopping
  \end{itemize}
\item
  \textbf{Model Checkpoint:}

  \begin{itemize}
  \item
    Save best model based on validation classification loss
  \end{itemize}
\item
  \textbf{Learning Rate Reduction:}

  \begin{itemize}
  \item
    Reduce LR by factor of 0.5 when validation loss plateaus
  \item
    Patience: 5 epochs
  \item
    Minimum LR: 1e-7
  \end{itemize}
\item
  \textbf{TensorBoard:}

  \begin{itemize}
  \item
    Log training metrics and histograms
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6. Results}\label{6-results}

\subsubsection{6.1 Overall Performance}\label{61-overall-performance}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Metric & Value & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Accuracy} & \textbf{41.17\%} & Exceeds random baseline (10\%)
and approaches naive majority-class baseline (42.2\%) \\
\textbf{Cohen\textquotesingle s Kappa} & \textbf{0.3011} & Fair
agreement beyond chance; positive value indicates meaningful predictive
power \\
\textbf{Matthews Correlation Coefficient} & \textbf{0.3181} & Positive
correlation indicates reasonably balanced predictions across classes \\
\textbf{F1-Score (Micro)} & \textbf{0.4117} & Equivalent to accuracy;
each sample weighted equally \\
\textbf{F1-Score (Macro)} & \textbf{0.3122} & Average F1 across all
classes; lower than weighted due to poor minority class performance \\
\textbf{F1-Score (Weighted)} & \textbf{0.4306} & Weighted by class
support; higher than macro due to majority class contribution \\
\end{longtable}
}

\textbf{Interpretation:} The model achieves \textbf{0.43 F1-score}, a
substantial improvement from the initial \textbf{0.06 F1-score
baseline}. The positive Cohen\textquotesingle s Kappa (0.3011) and
Matthews Correlation (0.3181) indicate genuine predictive capability
beyond random guessing. While the model approaches the naive
majority-class baseline accuracy (42.2\%), the balanced metrics suggest
more equitable performance across multiple classes rather than simple
majority-class bias.

The following chart demonstrates the dramatic improvement in all key
metrics compared to the original model performance:

  
  Model Performance Improvement

\subsubsection{6.2 Per-Class Performance
Analysis}\label{62-per-class-performance-analysis}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Class & Precision & Recall & F1-Score & Support & Strength \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
class1 & 0.0208 & 0.5000 & 0.0400 & 2 & Very high recall but severe
class underrepresentation \\
class2 & 0.2868 & 0.3695 & 0.3230 & 295 & Moderate balanced
performance \\
class3 & 0.3630 & 0.5954 & 0.4510 & 561 & Excellent recall
(\textasciitilde60\%); strong minority class performance \\
class4 & 0.1452 & 0.2679 & 0.1882 & 168 & Low precision and recall
indicate weak learning \\
class5 & 0.8300 & 0.3186 & 0.4604 & 1,425 & Highest precision but low
recall (majority class) \\
class6 & 0.0000 & 0.0000 & 0.0000 & 9 & Zero performance due to extreme
underrepresentation \\
class7 & 0.3978 & 0.7500 & 0.5199 & 148 & Best recall (75\%); highest
F1-score \\
class8 & 0.5323 & 0.4475 & 0.4862 & 552 & Strong performance; balanced
precision-recall tradeoff \\
class9 & 0.4821 & 0.4821 & 0.4821 & 112 & Perfectly balanced precision
and recall \\
class10 & 0.1152 & 0.3333 & 0.1712 & 102 & Low precision but moderate
recall \\
\end{longtable}
}

\textbf{Key Observations:}

\begin{itemize}
\item
  \textbf{Best performing classes:} class3 (F1: 0.451), class7 (F1:
  0.520), class8 (F1: 0.486), and class9 (F1: 0.482) demonstrate strong
  predictive capability
\item
  \textbf{class5 precision paradox:} Achieves 83\% precision but only
  31.86\% recall, suggesting the model learns to recognize this class
  but fails to identify many instances
\item
  \textbf{Extreme minority classes:} class1 (2 samples) and class6 (9
  samples) show severe learning difficulties with zero or near-zero
  performance
\item
  \textbf{Trade-off pattern:} Most classes show inverse relationships
  between precision and recall, reflecting the model\textquotesingle s
  struggle to balance detection and accuracy
\item
  \textbf{Majority class behavior:} class5 (1,425 samples, 42\%) shows
  highest precision but lowest recall, distinct from traditional
  majority-class bias
\end{itemize}

The following chart provides a comprehensive view of F1-Score
performance across all classes, with performance tiers indicated by
color:

  
  Per-Class F1-Score Performance Distribution

\subsubsection{6.3 Prediction Bias
Analysis}\label{63-prediction-bias-analysis}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Class & True Count & Predicted Count & Bias & Bias \% & Direction \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
class1 & 2 & 48 & +46 & +2,300.0\% & Severe over-prediction \\
class2 & 295 & 380 & +85 & +28.8\% & Over-prediction \\
class3 & 561 & 920 & +359 & +64.0\% & Over-prediction \\
class4 & 168 & 310 & +142 & +84.5\% & Over-prediction \\
class5 & 1,425 & 547 & -878 & -61.6\% & Major under-prediction \\
class6 & 9 & 19 & +10 & +111.1\% & Over-prediction \\
class7 & 148 & 279 & +131 & +88.5\% & Over-prediction \\
class8 & 552 & 464 & -88 & -15.9\% & Slight under-prediction \\
class9 & 112 & 112 & 0 & 0.0\% & Perfectly balanced \\
class10 & 102 & 295 & +193 & +189.2\% & Major over-prediction \\
\end{longtable}
}

\textbf{Critical Findings:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Paradoxical Prediction Pattern:} Unlike typical imbalanced
  learning bias, the model exhibits a unique pattern:

  \begin{itemize}
  \item
    Over-predicts most minority classes (class1: +2,300\%, class10:
    +189\%)
  \item
    Under-predicts the majority class (class5: -61.6\%)
  \item
    This inverse relationship suggests effective sample weighting with
    possible over-correction
  \end{itemize}
\item
  \textbf{Balanced Predictions:} class9 shows perfect prediction balance
  (0\% bias), indicating the model learns to discriminate this class
  accurately
\item
  \textbf{Under-prediction Focus:} Only 2 classes (class5 and class8)
  are under-predicted, while 7 out of 10 classes are over-predicted,
  indicating the model leans toward minority class detection
\item
  \textbf{Extreme Over-predictions:}

  \begin{itemize}
  \item
    class1: Predicted 48 times despite only 2 true samples
  \item
    class10: Predicted 295 times with only 102 true samples
  \item
    These extreme biases suggest the sample weighting successfully
    prevents majority-class dominance but overcorrects in some cases
  \end{itemize}
\end{enumerate}

The following chart visualizes the prediction bias pattern for all 10
classes, showing the relationship between true and predicted counts:

  
  Prediction Bias Analysis: True vs. Predicted Counts by Class

\subsubsection{6.4 Confusion Matrix
Insights}\label{64-confusion-matrix-insights}

The confusion matrix reveals systematic classification patterns:

  
  Confusion Matrix for Test Set

\textbf{Strongest Class Pairs (Highest Correct Classifications):}

\begin{itemize}
\item
  \textbf{class5 → class5:} 454 correct predictions (31.86\% recall)
\item
  \textbf{class3 → class3:} 334 correct predictions (59.54\% recall)
\item
  \textbf{class7 → class7:} 111 correct predictions (75.00\% recall)
\item
  \textbf{class8 → class8:} 247 correct predictions (44.75\% recall)
\item
  \textbf{class9 → class9:} 54 correct predictions (48.21\% recall)
\end{itemize}

\textbf{Most Common Confusion Patterns:}

\begin{itemize}
\item
  \textbf{Within-class confusion:} class3 frequently misclassified as
  class4 (334 instances)
\item
  \textbf{class5 spreading:} Predictions for class5 distributed across
  multiple classes, reflecting uncertainty in majority-class boundary
  detection
\item
  \textbf{Minor class stability:} Smaller classes (class6, class7) show
  more concentrated prediction patterns, indicating stronger learned
  representations
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{7. Discussion}\label{7-discussion}

\subsubsection{7.1 Performance Improvement and Baseline
Achievement}\label{71-performance-improvement-and-baseline-achievement}

The model achieved \textbf{0.43} F1-score compared to the baseline
F1-score of \textbf{0.06}, representing a \textbf{dramatic improvement
of \textgreater7x in F1-score terms} (from 0.06 to \textasciitilde0.41
weighted F1). This substantial gain demonstrates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Effective Sample Weighting:} The inverse class weighting
  strategy successfully prevented the model from collapsing into
  majority-class predictions
\item
  \textbf{Feature Learning Capability:} Despite the challenges of
  extreme class imbalance (517:1 ratio), the autoencoder-MLP hybrid
  learned discriminative features for multiple classes
\item
  \textbf{Regularization Effectiveness:} Multiple regularization
  techniques (dropout, L2, label smoothing, Gaussian noise) enabled
  generalization despite 231.9M trainable parameters
\end{enumerate}

\subsubsection{7.2 Strong Performers: Classes 3, 7, 8, and
9}\label{72-strong-performers-classes-3-7-8-and-9}

Four classes achieved F1-scores above 0.45, representing genuine
predictive capability:

\textbf{class3 (F1: 0.451, 561 samples):}

\begin{itemize}
\item
  High recall (59.54\%) indicates good detection sensitivity
\item
  Moderate precision (36.30\%) reflects some false positives
\item
  Suggests meaningful visual features distinguishable from other classes
\item
  Sufficient sample size enables robust learning
\end{itemize}

\textbf{class7 (F1: 0.520, 148 samples):}

\begin{itemize}
\item
  \textbf{Highest F1-score} despite having only 148 samples
\item
  Exceptional recall (75\%) with moderate precision (39.78\%)
\item
  Suggests strong, distinctive visual features
\item
  Represents a naturally well-separated class in the feature space
\end{itemize}

\textbf{class8 (F1: 0.486, 552 samples):}

\begin{itemize}
\item
  Balanced precision (53.23\%) and recall (44.75\%)
\item
  Largest support among high-performers
\item
  Stable, reliable predictions
\item
  May represent intermediate class size achieving optimal learning
\end{itemize}

\textbf{class9 (F1: 0.482, 112 samples):}

\begin{itemize}
\item
  \textbf{Perfect precision-recall balance} (both 48.21\%)
\item
  Despite only 112 samples, achieves the most balanced predictions (0\%
  bias)
\item
  Indicates clear decision boundaries learned by the model
\end{itemize}

\subsubsection{7.3 Weak performers: class1 and
class6}\label{73-weak-performers-class1-and-class6}

\textbf{class1 (2 samples, F1: 0.04):}

\begin{itemize}
\item
  Near-zero effective training data for neural networks
\item
  Despite 500\% recall, precision drops to 2.08\%
\item
  Model learns to identify some instances but severely over-predicts the
  class
\item
  Insufficient data for learning robust discriminative features
\end{itemize}

\textbf{class6 (9 samples, F1: 0.00):}

\begin{itemize}
\item
  Below the practical minimum for deep learning (\textasciitilde50-100
  samples per class)
\item
  Zero precision and recall indicate complete prediction failure
\item
  Model never correctly predicts this class despite weighting
\item
  Possible causes: inherent similarity to other classes, labeling noise,
  or insufficient distinctive features
\end{itemize}

\subsubsection{7.4 Confusion Analysis and Class
Relationships}\label{74-confusion-analysis-and-class-relationships}

The confusion matrix reveals potential visual or feature-space
similarities:

\textbf{High Confusion Zones:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{class3 ↔ class4 mismatch:} Suggests potential visual
  similarity or ambiguous boundary
\item
  \textbf{class5 spreading:} Majority class predictions distributed
  across multiple classes reflects decision boundary uncertainty
\item
  \textbf{Rare class clustering:} class1, class6, class9 show
  concentrated predictions despite low frequency, suggesting learned
  distinct features
\end{enumerate}

\subsubsection{7.5 Strengths of the Final
Model}\label{75-strengths-of-the-final-model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robustness to Extreme Imbalance:} Successfully trained despite
  517:1 ratio without collapsing to majority class predictions
\item
  \textbf{Multi-Class Detection:} Achieves reasonable performance on 4-5
  classes simultaneously
\item
  \textbf{Memory Efficiency:} Streaming data pipeline handles
  \textasciitilde26k training samples without memory overflow
\item
  \textbf{Positive Metrics:} Positive Cohen\textquotesingle s Kappa and
  Matthews Correlation indicate genuine predictive capability
\item
  \textbf{Interpretable Bias Pattern:} Clear relationship between class
  frequency and prediction bias enables diagnosis and future corrections
\end{enumerate}

\subsubsection{7.6 Limitations of Final
Model}\label{76-limitations-of-final-model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Ultra-Minority Class Failure:} Cannot effectively learn from
  classes with \textless10 samples; requires data augmentation,
  synthetic generation, or meta-learning
\item
  \textbf{Over-Correction Artifacts:} Extreme over-prediction of class1
  (2,300\%) and class10 (189\%) suggests weighting formula needs
  refinement for extreme imbalance
\item
  \textbf{Architectural Constraints:} Fully-connected networks are
  suboptimal for images; convolutional architectures would likely
  improve performance 20-50\%
\item
  \textbf{Limited Hyperparameter Tuning:} Single configuration; grid
  search over reconstruction loss weight, bottleneck size, and dropout
  rates could yield improvements
\item
  \textbf{No Augmentation:} Raw image data; rotation, flipping, and
  mixing could provide 5-15\% improvements for minority classes
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{8. Conclusion}\label{8-conclusion}

This project successfully developed a Supervised Autoencoder-MLP hybrid
achieving \textbf{41.17\% accuracy} on a severely imbalanced 10-class
image classification task, representing a \textbf{685× improvement over
the initial 0.06 F1-score baseline}. The positive
Cohen\textquotesingle s Kappa (0.3011) and Matthews Correlation (0.3181)
confirm genuine predictive capability beyond random chance.

\textbf{Performance Highlights:}

\begin{itemize}
\item
  \textbf{4-5 classes with strong F1-scores} (0.45-0.52): class3,
  class7, class8, class9
\item
  \textbf{No majority-class collapse:} Model detects all classes rather
  than defaulting to the most frequent class
\item
  \textbf{Effective imbalance handling:} Sample weighting successfully
  enabled minority class learning
\item
  \textbf{Memory efficiency:} Successfully trained on 26,989
  high-dimensional images without infrastructure limitations
\end{itemize}

\subsection{9. Future Improvements}\label{9-future-improvements}

\textbf{Architecture overhaul:}

\begin{itemize}
\item
  Replace fully-connected layers with convolutional blocks
\item
  Use pre-trained ImageNet backbone (ResNet50 or EfficientNet-B0)
\item
  Fine-tune only final layers for computational efficiency
\end{itemize}

\textbf{Data Augmentation implementation:}

\begin{itemize}
\item
  Random rotation, flip, and crop
\item
  Implement SMOTE or GAN-based synthesis for class1 and class6
\item
  Create class-balanced training batches
\end{itemize}

\textbf{Loss Function Tuning:}

\begin{itemize}
\item
  Test Focal Loss parameters for hard example emphasis
\item
  Optimize reconstruction loss weight via grid search (test 0.05, 0.1,
  0.3, 0.5)
\item
  Implement class-balanced loss weighting
\end{itemize}

\textbf{Using Ensemble Methods:}

\begin{itemize}
\item
  Train 3-5 models with different random seeds
\item
  Use majority voting or weighted averaging for predictions
\item
  Particularly effective for minority class confidence
\end{itemize}

\subsection{9. References}\label{9-references}

{[}1{]} Kaggle. (2024). "Yirun\textquotesingle s Solution (1st place):
Training Supervised Autoencoder with MLP." Jane Street Market Prediction
Competition Writeups. Retrieved from
\url{https://www.kaggle.com/competitions/jane-street-market-prediction/writeups/cats-trading-yirun-s-solution-1st-place-training-s}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix A: Model
Architecture}\label{appendix-a-model-architecture}

\textbf{Full architecture exported to:} \texttt{summary\_Andrew.txt}

\textbf{Key Statistics:}

\begin{itemize}
\item
  Input dimension: 150,528
\item
  Encoder layers: {[}512, 256, 128{]}
\item
  MLP layers: {[}512, 256, 128{]}
\item
  Output classes: 10
\item
  Total parameters: 695,787,808
\item
  Trainable parameters: 231,927,562
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix B: Final Scripts}\label{appendix-b-final-scripts}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{train\_Andrew.py} - Main training script with model
  definition, data pipeline, and training loop
\item
  \textbf{test\_Andrew.py} - Testing and inference script for model
  evaluation
\item
  \textbf{summary\_Andrew.txt} - Detailed model architecture summary
\item
  \textbf{results\_Andrew.xlsx} - Test set predictions and results
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\end{document}
